{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ImageSplicing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63fe14b208c14ec897724f253a20857b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_58063c528d964d1abd9fa601364b0430",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1d33f1afb20c4a609438c900cbeafe00",
              "IPY_MODEL_0b8b4ac8499d4286a000ee0d02be76c0"
            ]
          }
        },
        "58063c528d964d1abd9fa601364b0430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d33f1afb20c4a609438c900cbeafe00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_183321789d924ae09b817d62c995a855",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aaa679f71d43452180d3322c519ba8bd"
          }
        },
        "0b8b4ac8499d4286a000ee0d02be76c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_734a2174fc254aee9389c6e5fa5b13d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [03:56&lt;00:00, 1.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3d9fc9978fa4754896c5747fd3526d9"
          }
        },
        "183321789d924ae09b817d62c995a855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aaa679f71d43452180d3322c519ba8bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "734a2174fc254aee9389c6e5fa5b13d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3d9fc9978fa4754896c5747fd3526d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kuyas/ImageSplicing/blob/master/Copy_of_ImageSplicing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3GKjKFdwmqU",
        "colab_type": "text"
      },
      "source": [
        "# Image Splicing\n",
        "\n",
        "## Import The required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyHA-IwGIXbO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "af445173-4a91-4d0e-beed-c415959f0bcf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWwBRp6a_oPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet152\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn import svm\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchvision import datasets\n",
        "import statistics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETasjrKDwtk1",
        "colab_type": "text"
      },
      "source": [
        "## Configuration Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHtnLfmstvb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 1\n",
        "LEARNING_RATE = 0.0003\n",
        "ILLUMINANT_TRAIN_DATAPATH = '/content/drive/My Drive/DSO-1-Illuminants/train/'\n",
        "ILLUMINANT_VAL_DATAPATH = '/content/drive/My Drive/DSO-1-Illuminants/val/'\n",
        "ILLUMINANT_TEST_DATAPATH = '/content/drive/My Drive/DSO-1-Illuminants/test_2/'\n",
        "clf = svm.SVC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UbD9fMswxEy",
        "colab_type": "text"
      },
      "source": [
        "## The Resnet and Feature Extraction is defined here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBx_MB0GtNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Base(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Base, self).__init__()\n",
        "      self.model = models.resnet152(True)\n",
        "\n",
        "\n",
        "      # The feature extraction to create the DSF \n",
        "      self.feature_extract = nn.Sequential(self.model.conv1,\n",
        "                                           self.model.bn1,\n",
        "                                           nn.ReLU(),\n",
        "                                           nn.MaxPool2d(3,2,1,1,ceil_mode=False),\n",
        "                                           self.model.layer1,\n",
        "                                           self.model.layer2,\n",
        "                                           self.model.layer3,\n",
        "                                           self.model.layer4)\n",
        "      \n",
        "      self.avgpool = self.model.avgpool\n",
        "      self.classifier = self.model.fc\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.fc1 = nn.Linear(1000,500)\n",
        "      self.fc2 = nn.Linear(500,250)\n",
        "      self.fc3 = nn.Linear(250,2)\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "      self.gradients = None\n",
        "    \n",
        "    # The hook to get the gradient at the particular layer\n",
        "    def activations_hook(self,grad):\n",
        "      self.gradients = grad\n",
        "    \n",
        "    def get_gradient(self):\n",
        "        return self.gradients\n",
        "\n",
        "    def get_activations_hook(self):\n",
        "      return self.gradients\n",
        "\n",
        "    def get_activations(self,x):\n",
        "      return self.feature_extract(x)\n",
        "      \n",
        "    def forward(self,x):\n",
        "      x = self.feature_extract(x)\n",
        "      h = x.register_hook(self.activations_hook)\n",
        "      x = self.avgpool(x)\n",
        "      x = x.view((1,-1))\n",
        "      y = x\n",
        "      x = self.classifier(x)\n",
        "      # x = self.dropout(F.relu(self.fc1(x)))\n",
        "      # x = self.dropout(F.relu(self.fc2(x)))\n",
        "      # x = self.dropout(F.relu(self.fc3(x)))\n",
        "      # x = self.softmax(x)\n",
        "      return x,y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2jh2hBZB9Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "    \"\"\"Custom dataset that includes image file paths. Extends\n",
        "    torchvision.datasets.ImageFolder\n",
        "    \"\"\"\n",
        "\n",
        "    # override the __getitem__ method. this is the method that dataloader calls\n",
        "    def __getitem__(self, index):\n",
        "        # this is what ImageFolder normally returns \n",
        "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "        # the image file path\n",
        "        path = self.imgs[index][0]\n",
        "        # make a new tuple that includes original and the path\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "        return tuple_with_path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkehEMt0wUvB",
        "colab_type": "text"
      },
      "source": [
        "## Transformations for the Images\n",
        "- Resize images to 224x224 (The input to the ResNet is 224x224)\n",
        "- convert to a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGNyMN-UtzDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_TRANSFORM_IMG = transforms.Compose([\n",
        "    transforms.Resize(size=224),\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
        "    # transforms.RandomRotation(degrees = 15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "VAL_TRANSFORM_IMG = transforms.Compose([\n",
        "    transforms.Resize(size=224),\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
        "    # transforms.CenterCrop(size=224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "train_data = ImageFolderWithPaths(root=ILLUMINANT_TRAIN_DATAPATH, transform=TRAIN_TRANSFORM_IMG)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
        "\n",
        "\n",
        "data = ImageFolderWithPaths(root=ILLUMINANT_TRAIN_DATAPATH, transform=TRAIN_TRANSFORM_IMG)\n",
        "data_loader = torch.utils.data.DataLoader(train_data,batch_size=1,shuffle=True,num_workers=0)\n",
        "\n",
        "\n",
        "val_data = ImageFolderWithPaths(root=ILLUMINANT_VAL_DATAPATH, transform=VAL_TRANSFORM_IMG)\n",
        "val_data_loader = torch.utils.data.DataLoader(val_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
        "\n",
        "test_data = ImageFolderWithPaths(root=ILLUMINANT_TEST_DATAPATH, transform=VAL_TRANSFORM_IMG)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCr_1sBpxEx8",
        "colab_type": "text"
      },
      "source": [
        "## Information about the datase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HfQthNJt0lP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "1cba06e2-c7ca-4944-d7d7-779c3498c57e"
      },
      "source": [
        "dataiter = iter(train_data_loader)\n",
        "images, labels, paths = dataiter.next()\n",
        "print(\"Number of Training Examples: \", len(train_data))\n",
        "print(\"Number of Test Examples: \", len(test_data))\n",
        "print(\"Number of Validation Examples: \", len(val_data))\n",
        "print(\"Detected Classes are: \", train_data.class_to_idx)\n",
        "train_iter = iter(train_data_loader)\n",
        "images, labels, _ = train_iter.next()\n",
        "print(\"Image Shape on Batch size = {} \".format(images.size()))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Examples:  160\n",
            "Number of Test Examples:  50\n",
            "Number of Validation Examples:  40\n",
            "Detected Classes are:  {'normal': 0, 'spliced': 1}\n",
            "Image Shape on Batch size = torch.Size([1, 3, 224, 224]) \n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM-MAvW3xXQl",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the model\n",
        "- create model\n",
        "- transfer it to the GPU\n",
        "- define the loss function (CrossEntropyLoss)\n",
        "- Adam Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXeqettNxZZ4",
        "colab_type": "text"
      },
      "source": [
        "## Function to generate the Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIVUPgeG1WY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def HeatmapGeneration(img,paths):\n",
        "  resnet.eval()\n",
        "  # img, _ = next(iter(data_loader))\n",
        "  pred,_ = resnet(img)\n",
        "  # print(pred.shape)\n",
        "  pred[:, 2].backward()\n",
        "\n",
        "  \"\"\"\n",
        "  get the gradient and neuron values from the penultimate layer\n",
        "  We take the ones with the greatest change and build the heatmap from it\n",
        "  \"\"\"\n",
        "  gradients = resnet.get_gradient()\n",
        "  pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "  activations = resnet.get_activations(img).detach()\n",
        "\n",
        "  for i in range(512):\n",
        "      activations[:, i, :, :] *= pooled_gradients[i]\n",
        "\n",
        "      \n",
        "  heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "  heatmap = np.maximum(heatmap.cpu(), 0)\n",
        "  heatmap /= torch.max(heatmap)\n",
        "  # plt.matshow(heatmap.squeeze())\n",
        "  heatmap = heatmap.numpy()\n",
        "  paths = ''.join(paths)\n",
        "  p = paths.split(\"/\")\n",
        "  img = cv2.imread(paths)\n",
        "\n",
        "  \"\"\"\n",
        "  Resizing the heatmap to fit the image\n",
        "  \"\"\"\n",
        "  heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "  heatmap = np.uint8(255 * heatmap)\n",
        "  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_HSV)\n",
        "  superimposed_img = heatmap * 0.4 + img\n",
        "  scale_percent = 20 # percent of original size\n",
        "  width = int(superimposed_img.shape[1] * scale_percent / 100)\n",
        "  height = int(superimposed_img.shape[0] * scale_percent / 100)\n",
        "  dim = (width, height)\n",
        "  resized = cv2.resize(superimposed_img, dim, interpolation = cv2.INTER_AREA)\n",
        "  # cv2_imshow(resized)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Convert the heatmap to HSV space and display only the H\n",
        "  \"\"\"\n",
        "  h, s, v = resized[:, :, 0], resized[:, :, 1], resized[:, :, 2]\n",
        "  # cv2_imshow(h)\n",
        "  resized = np.float32(h)\n",
        "  hist = cv2.calcHist([resized], [0], None, [256], [0, 256])\n",
        "  # cv2_imshow(hist)\n",
        "  # plt.show()\n",
        "  print(p[-2],min(hist),max(hist),np.mean(hist),np.median(hist))\n",
        "  return max(hist)\n",
        "  # print(hist.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5dXpinM0xLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "63fe14b208c14ec897724f253a20857b",
            "58063c528d964d1abd9fa601364b0430",
            "1d33f1afb20c4a609438c900cbeafe00",
            "0b8b4ac8499d4286a000ee0d02be76c0",
            "183321789d924ae09b817d62c995a855",
            "aaa679f71d43452180d3322c519ba8bd",
            "734a2174fc254aee9389c6e5fa5b13d0",
            "e3d9fc9978fa4754896c5747fd3526d9"
          ]
        },
        "outputId": "bea76998-305d-457e-c931-03d6db5042c7"
      },
      "source": [
        "resnet = Base()\n",
        "resnet.to(device)\n",
        "resnet.eval()\n",
        "dsf = []\n",
        "y_dsf = []\n",
        "k = 0\n",
        "t_iter = iter(data_loader)\n",
        "while k in range(160):\n",
        "  k=k+1\n",
        "  img,label,paths = t_iter.next()\n",
        "  img = img.cuda()\n",
        "  pred, features = resnet(img)\n",
        "  pred.argmax(dim=1)\n",
        "  pred[:,2].backward()\n",
        "  gradients = resnet.get_gradient()\n",
        "  pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "  activations = resnet.get_activations(img).detach()\n",
        "  dsf.append(features.cpu().data.numpy())\n",
        "  # p = paths.raw_input().split(\"/\")\n",
        "  paths = ''.join(paths)\n",
        "  p = paths.split(\"/\")\n",
        "  if p[-2] == 'spliced':\n",
        "      y_dsf.append(0)\n",
        "  else:\n",
        "      y_dsf.append(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63fe14b208c14ec897724f253a20857b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se-UntVc_JtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dsf = np.asarray(dsf)\n",
        "dsf = dsf.reshape(160,2048)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NP2ud1K7bx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "be542560-8d5b-4a93-ed78-b7ac7a57e2e2"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC(gamma='auto')\n",
        "clf.fit(dsf, y_dsf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aJzPjXZ_-AE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "469170d1-76af-4ce8-9a0a-f1e742e9ec6b"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "classes =['spliced','normal']\n",
        "# confusion_matrix = torch.zeros(2, 2)\n",
        "correct_0 = 0\n",
        "correct_1 = 0\n",
        "class_correct = list(0. for i in range(2))\n",
        "class_total = list(0. for i in range(2))\n",
        "mean = 0\n",
        "resnet.eval()\n",
        "# with torch.no_grad():\n",
        "for data in test_data_loader:\n",
        "    total=total+1\n",
        "    images,labels,paths = data[0].to(device), data[1].to(device),data[2]\n",
        "    outputs, features = resnet(images)\n",
        "\n",
        "    predicted = clf.predict(features.cpu().data.numpy())\n",
        "    paths = ''.join(paths)\n",
        "    p = paths.split(\"/\")\n",
        "    actual = 0\n",
        "    if p[-2] == 'spliced':\n",
        "        actual = 0\n",
        "        class_total[0] += 1\n",
        "    else:\n",
        "        actual = 1\n",
        "        class_total[1]+=1\n",
        "\n",
        "    if(predicted == 0):\n",
        "      mean += HeatmapGeneration(images,paths)\n",
        "    if(actual == predicted and actual == 0):\n",
        "      correct = correct+1\n",
        "      class_correct[0] +=1\n",
        "    elif(actual == predicted and actual == 1):\n",
        "      correct = correct+1\n",
        "      class_correct[1] +=1\n",
        "\n",
        "\n",
        "acc = 100*correct/total\n",
        "print(correct)\n",
        "print(total)\n",
        "print('Testing accuracy: ' + str(acc))\n",
        "# print(confusion_matrix)\n",
        "\n",
        "for i in range(2):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "acc0 = 100 * class_correct[0] / class_total[0]\n",
        "acc1 = 100 * class_correct[1] / class_total[1]\n",
        "print_row_end = []\n",
        "row_end = [acc,acc0,acc1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spliced [0.] [454.] 31.640625 22.0\n",
            "spliced [0.] [628.] 36.09375 27.5\n",
            "spliced [0.] [593.] 54.75 49.5\n",
            "spliced [0.] [884.] 38.898438 21.0\n",
            "spliced [0.] [4572.] 129.46875 29.5\n",
            "spliced [0.] [2920.] 86.18359 64.0\n",
            "spliced [0.] [643.] 60.097656 53.5\n",
            "spliced [0.] [2332.] 96.9375 50.0\n",
            "spliced [0.] [1930.] 96.78906 75.5\n",
            "spliced [0.] [3340.] 106.78125 86.0\n",
            "normal [0.] [3679.] 264.53125 176.5\n",
            "spliced [0.] [1174.] 72.49609 44.5\n",
            "spliced [0.] [2000.] 75.0 33.5\n",
            "spliced [0.] [1779.] 72.55469 48.5\n",
            "spliced [0.] [2529.] 96.9375 33.5\n",
            "spliced [0.] [1378.] 96.9375 66.0\n",
            "normal [0.] [294.] 16.417969 12.0\n",
            "spliced [0.] [12332.] 243.3711 120.0\n",
            "spliced [0.] [465.] 39.46875 28.0\n",
            "normal [0.] [3864.] 162.10938 83.5\n",
            "spliced [0.] [18863.] 265.40625 110.5\n",
            "spliced [0.] [1385.] 51.40625 32.5\n",
            "spliced [0.] [512.] 28.125 22.0\n",
            "spliced [0.] [280.] 12.0 6.0\n",
            "spliced [0.] [2791.] 86.703125 13.0\n",
            "44\n",
            "50\n",
            "Testing accuracy: 88.0\n",
            "Accuracy of spliced : 88 %\n",
            "Accuracy of normal : 88 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTvceGOZE7K2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b08390be-f5f1-407b-b063-eb5b7b85786b"
      },
      "source": [
        "mean/40"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11016.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnOEJaxDx8Qg",
        "colab_type": "text"
      },
      "source": [
        "## The running of the network\n",
        "- Forward Propogation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZkj64psMhUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Base()\n",
        "model.to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "# optimizer = optim.SGD(_params, lr=LEARNING_RATE, momentum=0.9)\n",
        "optimizer = optim.Adam(_params, lr=LEARNING_RATE, weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UR2lAGKt2Uw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "e223e92a-9636-49e9-b84a-29ce44332021"
      },
      "source": [
        "train_loss = []\n",
        "test_loss = []\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    correct = 0 \n",
        "    iterations = 0\n",
        "    iter_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i,data in enumerate(train_data_loader,0):\n",
        "        inputs, labels,paths = data[0].to(device), data[1].to(device),data[2]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)        \n",
        "        loss = criterion(outputs,labels)\n",
        "        iter_loss+=loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum()\n",
        "        iterations += 1\n",
        "\n",
        "    train_loss.append(iter_loss/iterations)\n",
        "    train_accuracy.append((100 * correct / len(train_data)))\n",
        "\n",
        "    # loss = 0.0\n",
        "    # correct = 0\n",
        "    # iterations = 0\n",
        "    # model.eval()\n",
        "\n",
        "    # for i, data in enumerate(val_data_loader):\n",
        "    #     inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    #     outputs = model(inputs)     \n",
        "    #     loss = criterion(outputs, labels)\n",
        "    #     loss += loss.item()\n",
        "    #     _, predicted = torch.max(outputs, 1)\n",
        "    #     correct += (predicted == labels).sum()\n",
        "\n",
        "    #     iterations+=1\n",
        "\n",
        "    # test_loss.append(loss/iterations)\n",
        "    # # Record the Testing accuracy\n",
        "    # test_accuracy.append((100 * correct / len(val_data)))\n",
        "    stop = time.time()\n",
        "    # print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Validation Loss: {:.3f}, Validation Acc: {:.3f}, Time: {:.3f}s'\n",
        "    #            .format(epoch+1, EPOCHS, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1], stop-start))\n",
        "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Time: {:.3f}s'\n",
        "               .format(epoch+1, EPOCHS, train_loss[-1], train_accuracy[-1], stop-start))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Training Loss: 0.695, Training Accuracy: 49.000, Time: 32.596s\n",
            "Epoch 2/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.603s\n",
            "Epoch 3/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.180s\n",
            "Epoch 4/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.304s\n",
            "Epoch 5/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.513s\n",
            "Epoch 6/20, Training Loss: 0.694, Training Accuracy: 49.000, Time: 32.400s\n",
            "Epoch 7/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.418s\n",
            "Epoch 8/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.349s\n",
            "Epoch 9/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.183s\n",
            "Epoch 10/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.437s\n",
            "Epoch 11/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.515s\n",
            "Epoch 12/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.605s\n",
            "Epoch 13/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.379s\n",
            "Epoch 14/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 31.929s\n",
            "Epoch 15/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.268s\n",
            "Epoch 16/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.202s\n",
            "Epoch 17/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.194s\n",
            "Epoch 18/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.226s\n",
            "Epoch 19/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.264s\n",
            "Epoch 20/20, Training Loss: 0.693, Training Accuracy: 50.000, Time: 32.578s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kZ0Pf8yDkD",
        "colab_type": "text"
      },
      "source": [
        "## Testing of the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux0HeRGat5Tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "classes =['NORMAL','SPLICING']\n",
        "# confusion_matrix = torch.zeros(2, 2)\n",
        "correct_0 = 0\n",
        "correct_1 = 0\n",
        "class_correct = list(0. for i in range(2))\n",
        "class_total = list(0. for i in range(2))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for data in test_data_loader:\n",
        "      images,labels,paths = data[0].to(device), data[1].to(device),data[2]\n",
        "      outputs, op_1 = model(images)\n",
        "      _, predicted = torch.max(outputs,1)\n",
        "      total += labels.size(0)\n",
        "      c = (predicted == labels).squeeze()\n",
        "      for i in range(BATCH_SIZE):\n",
        "        if(predicted == 1):\n",
        "          HeatmapGeneration(images,paths)\n",
        "      for i in range(4):\n",
        "          # print(labels)\n",
        "          label = labels\n",
        "          # print(c)\n",
        "          class_correct[label] += c.item()\n",
        "          class_total[label] += 1\n",
        "      correct+=(predicted == labels).sum().item()\n",
        "      for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "              confusion_matrix[t.long(), p.long()] += 1\n",
        "acc = 100*correct/total\n",
        "print(correct)\n",
        "print(total)\n",
        "print('Testing accuracy: ' + str(acc))\n",
        "# print(confusion_matrix)\n",
        "\n",
        "for i in range(2):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "acc0 = 100 * class_correct[0] / class_total[0]\n",
        "acc1 = 100 * class_correct[1] / class_total[1]\n",
        "print_row_end = []\n",
        "row_end = [acc,acc0,acc1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjDbMROBIPZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}