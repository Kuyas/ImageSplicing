{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageSplicing VGG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kuyas/ImageSplicing/blob/master/ImageSplicing_VGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3GKjKFdwmqU",
        "colab_type": "text"
      },
      "source": [
        "# Image Splicing\n",
        "\n",
        "## Import The required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyHA-IwGIXbO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f31101d-908e-42d4-c654-22695906f0e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWwBRp6a_oPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet152\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn import svm\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchvision import datasets\n",
        "import statistics"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETasjrKDwtk1",
        "colab_type": "text"
      },
      "source": [
        "## Configuration Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHtnLfmstvb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 1\n",
        "LEARNING_RATE = 0.0003\n",
        "ILLUMINANT_TRAIN_DATAPATH = '/content/drive/My Drive/DSO-1-Illuminants/train/'\n",
        "ILLUMINANT_VAL_DATAPATH = '/content/drive/My Drive/DSO-1-Illuminants/val/'\n",
        "ILLUMINANT_TEST_DATAPATH = '/content/drive/My Drive/DSO-1-Illuminants/test_2/'\n",
        "clf = svm.SVC()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UbD9fMswxEy",
        "colab_type": "text"
      },
      "source": [
        "## The Resnet and Feature Extraction is defined here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBx_MB0GtNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        \n",
        "        # get the pretrained VGG19 network\n",
        "        self.vgg = models.vgg19(pretrained=True)\n",
        "        \n",
        "        # disect the network to access its last convolutional layer\n",
        "        self.features_conv = self.vgg.features[:36]\n",
        "        \n",
        "        # get the max pool of the features stem\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        \n",
        "        # get the classifier of the vgg19\n",
        "        self.classifier = self.vgg.classifier\n",
        "        \n",
        "        # placeholder for the gradients\n",
        "        self.gradients = None\n",
        "    \n",
        "    # hook for the gradients of the activations\n",
        "    def activations_hook(self, grad):\n",
        "        self.gradients = grad\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features_conv(x)\n",
        "        \n",
        "        # register the hook\n",
        "        h = x.register_hook(self.activations_hook)\n",
        "        \n",
        "        # apply the remaining pooling\n",
        "        x = self.max_pool(x)\n",
        "        x = x.view((1, -1))\n",
        "        y=x\n",
        "        x = self.classifier(x)\n",
        "        return x,y\n",
        "    \n",
        "    # method for the gradient extraction\n",
        "    def get_activations_gradient(self):\n",
        "        return self.gradients\n",
        "    \n",
        "    # method for the activation exctraction\n",
        "    def get_activations(self, x):\n",
        "        return self.features_conv(x)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2jh2hBZB9Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "    \"\"\"Custom dataset that includes image file paths. Extends\n",
        "    torchvision.datasets.ImageFolder\n",
        "    \"\"\"\n",
        "\n",
        "    # override the __getitem__ method. this is the method that dataloader calls\n",
        "    def __getitem__(self, index):\n",
        "        # this is what ImageFolder normally returns \n",
        "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "        # the image file path\n",
        "        path = self.imgs[index][0]\n",
        "        # make a new tuple that includes original and the path\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "        return tuple_with_path\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkehEMt0wUvB",
        "colab_type": "text"
      },
      "source": [
        "## Transformations for the Images\n",
        "- Resize images to 224x224 (The input to the ResNet is 224x224)\n",
        "- convert to a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGNyMN-UtzDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_TRANSFORM_IMG = transforms.Compose([\n",
        "    transforms.Resize(size=224),\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
        "    # transforms.RandomRotation(degrees = 15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "VAL_TRANSFORM_IMG = transforms.Compose([\n",
        "    transforms.Resize(size=224),\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
        "    # transforms.CenterCrop(size=224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "train_data = ImageFolderWithPaths(root=ILLUMINANT_TRAIN_DATAPATH, transform=TRAIN_TRANSFORM_IMG)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
        "\n",
        "\n",
        "data = ImageFolderWithPaths(root=ILLUMINANT_TRAIN_DATAPATH, transform=TRAIN_TRANSFORM_IMG)\n",
        "data_loader = torch.utils.data.DataLoader(train_data,batch_size=1,shuffle=True,num_workers=0)\n",
        "\n",
        "\n",
        "val_data = ImageFolderWithPaths(root=ILLUMINANT_VAL_DATAPATH, transform=VAL_TRANSFORM_IMG)\n",
        "val_data_loader = torch.utils.data.DataLoader(val_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
        "\n",
        "test_data = ImageFolderWithPaths(root=ILLUMINANT_TEST_DATAPATH, transform=VAL_TRANSFORM_IMG)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCr_1sBpxEx8",
        "colab_type": "text"
      },
      "source": [
        "## Information about the datase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HfQthNJt0lP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "a09ce556-0bed-4187-ca33-3a6d07dec8cb"
      },
      "source": [
        "dataiter = iter(train_data_loader)\n",
        "images, labels, paths = dataiter.next()\n",
        "print(\"Number of Training Examples: \", len(train_data))\n",
        "print(\"Number of Test Examples: \", len(test_data))\n",
        "print(\"Number of Validation Examples: \", len(val_data))\n",
        "print(\"Detected Classes are: \", train_data.class_to_idx)\n",
        "train_iter = iter(train_data_loader)\n",
        "images, labels, _ = train_iter.next()\n",
        "print(\"Image Shape on Batch size = {} \".format(images.size()))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Examples:  160\n",
            "Number of Test Examples:  50\n",
            "Number of Validation Examples:  40\n",
            "Detected Classes are:  {'normal': 0, 'spliced': 1}\n",
            "Image Shape on Batch size = torch.Size([1, 3, 224, 224]) \n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM-MAvW3xXQl",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the model\n",
        "- create model\n",
        "- transfer it to the GPU\n",
        "- define the loss function (CrossEntropyLoss)\n",
        "- Adam Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXeqettNxZZ4",
        "colab_type": "text"
      },
      "source": [
        "## Function to generate the Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIVUPgeG1WY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def HeatmapGeneration(img,paths):\n",
        "  resnet.eval()\n",
        "  # img, _ = next(iter(data_loader))\n",
        "  pred,_ = resnet(img)\n",
        "  # print(pred.shape)\n",
        "  pred[:, 2].backward()\n",
        "\n",
        "  \"\"\"\n",
        "  get the gradient and neuron values from the penultimate layer\n",
        "  We take the ones with the greatest change and build the heatmap from it\n",
        "  \"\"\"\n",
        "  gradients = resnet.get_activations_gradient()\n",
        "  pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "  activations = resnet.get_activations(img).detach()\n",
        "\n",
        "  for i in range(512):\n",
        "      activations[:, i, :, :] *= pooled_gradients[i]\n",
        "\n",
        "      \n",
        "  heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "  heatmap = np.maximum(heatmap.cpu(), 0)\n",
        "  heatmap /= torch.max(heatmap)\n",
        "  # plt.matshow(heatmap.squeeze())\n",
        "  heatmap = heatmap.numpy()\n",
        "  paths = ''.join(paths)\n",
        "  p = paths.split(\"/\")\n",
        "  img = cv2.imread(paths)\n",
        "\n",
        "  \"\"\"\n",
        "  Resizing the heatmap to fit the image\n",
        "  \"\"\"\n",
        "  heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "  heatmap = np.uint8(255 * heatmap)\n",
        "  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_HSV)\n",
        "  superimposed_img = heatmap * 0.4 + img\n",
        "  scale_percent = 20 # percent of original size\n",
        "  width = int(superimposed_img.shape[1] * scale_percent / 100)\n",
        "  height = int(superimposed_img.shape[0] * scale_percent / 100)\n",
        "  dim = (width, height)\n",
        "  resized = cv2.resize(superimposed_img, dim, interpolation = cv2.INTER_AREA)\n",
        "  # cv2_imshow(resized)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Convert the heatmap to HSV space and display only the H\n",
        "  \"\"\"\n",
        "  h, s, v = resized[:, :, 0], resized[:, :, 1], resized[:, :, 2]\n",
        "  # cv2_imshow(h)\n",
        "  resized = np.float32(h)\n",
        "  hist = cv2.calcHist([resized], [0], None, [256], [0, 256])\n",
        "  # cv2_imshow(hist)\n",
        "  # plt.show()\n",
        "  # if(p[-2] == 'tampered'):\n",
        "  print(p[-2],min(hist),max(hist),np.mean(hist),np.median(hist))\n",
        "  return max(hist)\n",
        "  # print(hist.shape)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5dXpinM0xLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet = VGG()\n",
        "resnet.to(device)\n",
        "resnet.eval()\n",
        "dsf = []\n",
        "y_dsf = []\n",
        "k = 0\n",
        "t_iter = iter(data_loader)\n",
        "while k in range(160):\n",
        "  k=k+1\n",
        "  # print(k)\n",
        "  img,label,paths = t_iter.next()\n",
        "  img = img.cuda()\n",
        "  pred, features = resnet(img)\n",
        "  pred.argmax(dim=1)\n",
        "  pred[:,2].backward()\n",
        "  gradients = resnet.get_activations_gradient()\n",
        "  pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "  activations = resnet.get_activations(img).detach()\n",
        "  dsf.append(features.cpu().data.numpy())\n",
        "  # p = paths.raw_input().split(\"/\")\n",
        "  paths = ''.join(paths)\n",
        "  p = paths.split(\"/\")\n",
        "  if p[-2] == 'spliced':\n",
        "      y_dsf.append(0)\n",
        "  else:\n",
        "      y_dsf.append(1)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se-UntVc_JtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dsf = np.asarray(dsf)\n",
        "dsf = dsf.reshape(160,-1)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NP2ud1K7bx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "a574faaf-1820-461a-8018-d5cac7b9678c"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "# clf = SVC(gamma='auto',class_weight={0:0.25,1:0.75})\n",
        "clf = SVC(gamma='auto',class_weight=\"balanced\")\n",
        "clf.fit(dsf, y_dsf)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aJzPjXZ_-AE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "be3004b6-a66b-47de-e1dd-e687b86e06d7"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "classes =['spliced','normal']\n",
        "confusion_matrix = torch.zeros(2, 2)\n",
        "correct_0 = 0\n",
        "correct_1 = 0\n",
        "count = 0\n",
        "class_correct = list(0. for i in range(2))\n",
        "class_total = list(0. for i in range(2))\n",
        "mean = 0\n",
        "threshold = 828\n",
        "resnet.eval()\n",
        "# with torch.no_grad():\n",
        "for data in test_data_loader:\n",
        "    total=total+1\n",
        "    images,labels,paths = data[0].to(device), data[1].to(device),data[2]\n",
        "    # images = to_pil(gray_world(from_pil(images))\n",
        "    outputs, features = resnet(images)\n",
        "\n",
        "    predicted = clf.predict(features.cpu().data.numpy())\n",
        "    paths = ''.join(paths)\n",
        "    p = paths.split(\"/\")\n",
        "    actual = 0\n",
        "    if p[-2] == 'spliced':\n",
        "        actual = 0\n",
        "        class_total[0] += 1\n",
        "    else:\n",
        "        actual = 1\n",
        "        class_total[1]+=1\n",
        "\n",
        "    # if(predicted == 1):\n",
        "    #   print(\"1\",end=\" \")\n",
        "    #   count = count+1\n",
        "    #   mean_single = HeatmapGeneration(images,paths)\n",
        "    #   mean+=mean_single\n",
        "    #   if mean_single < 900:\n",
        "    #     predicted = 1\n",
        "    # if(predicted == 0):\n",
        "    #   print(\"0\",end=\" \")\n",
        "    #   count = count+1\n",
        "    #   mean_single = HeatmapGeneration(images,paths)\n",
        "    #   mean+=mean_single\n",
        "    #   if mean_single > 1100:\n",
        "    #     predicted = 1\n",
        "    if(actual == predicted and actual == 0):\n",
        "      correct = correct+1\n",
        "      class_correct[0] +=1\n",
        "    elif(actual == predicted and actual == 1):\n",
        "      correct = correct+1\n",
        "      class_correct[1] +=1\n",
        "\n",
        "\n",
        "\n",
        "# print(mean,count,mean/count)\n",
        "\n",
        "acc = 100*correct/total\n",
        "print(correct)\n",
        "print(total)\n",
        "print('Testing accuracy: ' + str(acc))\n",
        "\n",
        "\n",
        "for i in range(2):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "print(\"0\",class_correct[0],class_total[0]-class_correct[0])\n",
        "print(\"1\",class_correct[1],class_total[1]-class_correct[1])\n",
        "acc0 = 100 * class_correct[0] / class_total[0]\n",
        "acc1 = 100 * class_correct[1] / class_total[1]\n",
        "print_row_end = []\n",
        "row_end = [acc,acc0,acc1]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41\n",
            "50\n",
            "Testing accuracy: 82.0\n",
            "Accuracy of spliced : 80 %\n",
            "Accuracy of normal : 84 %\n",
            "0 20.0 5.0\n",
            "1 21.0 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTvceGOZE7K2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "b1015598-4a03-4508-bede-3e3063c5ef10"
      },
      "source": [
        "Testing accuracy: 77.77777777777777\n",
        "Accuracy of tampered : 89 %\n",
        "Accuracy of normal : 50 %\n",
        "threshold - < 700"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-aad6a3669dbd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Testing accuracy: 77.77777777777777\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0F2XjWlAmZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Testing accuracy: 68.57142857142857\n",
        "Accuracy of tampered : 61 %\n",
        "Accuracy of normal : 76 %\n",
        "threshold - > 900\n",
        "\n",
        "Testing accuracy: 68.57142857142857\n",
        "Accuracy of tampered : 83 %\n",
        "Accuracy of normal : 52 %\n",
        "threshold - > 1100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmBaxttpAOlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" GRAY WORLD\"\"\"\n",
        "Testing accuracy: 62.857142857142854\n",
        "Accuracy of tampered : 44 %\n",
        "Accuracy of normal : 82 %\n",
        "threshold = 750\n",
        "\n",
        "Testing accuracy: 65.71428571428571\n",
        "Accuracy of tampered : 50 %\n",
        "Accuracy of normal : 82 %\n",
        "threshold = 800"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnOEJaxDx8Qg",
        "colab_type": "text"
      },
      "source": [
        "## The running of the network\n",
        "- Forward Propogation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZkj64psMhUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Base()\n",
        "model.to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "# optimizer = optim.SGD(_params, lr=LEARNING_RATE, momentum=0.9)\n",
        "optimizer = optim.Adam(_params, lr=LEARNING_RATE, weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UR2lAGKt2Uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = []\n",
        "test_loss = []\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(epoch)\n",
        "    start = time.time()\n",
        "    correct = 0 \n",
        "    iterations = 0\n",
        "    iter_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i,data in enumerate(train_data_loader,0):\n",
        "        inputs, labels,paths = data[0].to(device), data[1].to(device),data[2]\n",
        "        optimizer.zero_grad()\n",
        "        outputs,_ = model(inputs)        \n",
        "        loss = criterion(outputs,labels)\n",
        "        iter_loss+=loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum()\n",
        "        iterations += 1\n",
        "\n",
        "    train_loss.append(iter_loss/iterations)\n",
        "    train_accuracy.append((100 * correct / len(train_data)))\n",
        "\n",
        "    # loss = 0.0\n",
        "    # correct = 0\n",
        "    # iterations = 0\n",
        "    # model.eval()\n",
        "\n",
        "    # for i, data in enumerate(val_data_loader):\n",
        "    #     inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    #     outputs = model(inputs)     \n",
        "    #     loss = criterion(outputs, labels)\n",
        "    #     loss += loss.item()\n",
        "    #     _, predicted = torch.max(outputs, 1)\n",
        "    #     correct += (predicted == labels).sum()\n",
        "\n",
        "    #     iterations+=1\n",
        "\n",
        "    # test_loss.append(loss/iterations)\n",
        "    # # Record the Testing accuracy\n",
        "    # test_accuracy.append((100 * correct / len(val_data)))\n",
        "    stop = time.time()\n",
        "    # print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Validation Loss: {:.3f}, Validation Acc: {:.3f}, Time: {:.3f}s'\n",
        "    #            .format(epoch+1, EPOCHS, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1], stop-start))\n",
        "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Time: {:.3f}s'\n",
        "               .format(epoch+1, EPOCHS, train_loss[-1], train_accuracy[-1], stop-start))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kZ0Pf8yDkD",
        "colab_type": "text"
      },
      "source": [
        "## Testing of the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux0HeRGat5Tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "classes =['NORMAL','SPLICING']\n",
        "# confusion_matrix = torch.zeros(2, 2)\n",
        "correct_0 = 0\n",
        "correct_1 = 0\n",
        "class_correct = list(0. for i in range(2))\n",
        "class_total = list(0. for i in range(2))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for data in test_data_loader:\n",
        "      images,labels,paths = data[0].to(device), data[1].to(device),data[2]\n",
        "      outputs, op_1 = model(images)\n",
        "      _, predicted = torch.max(outputs,1)\n",
        "      total += labels.size(0)\n",
        "      c = (predicted == labels).squeeze()\n",
        "      for i in range(BATCH_SIZE):\n",
        "        if(predicted == 1):\n",
        "          HeatmapGeneration(images,paths)\n",
        "      for i in range(4):\n",
        "          # print(labels)\n",
        "          label = labels\n",
        "          # print(c)\n",
        "          class_correct[label] += c.item()\n",
        "          class_total[label] += 1\n",
        "      correct+=(predicted == labels).sum().item()\n",
        "      for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "              confusion_matrix[t.long(), p.long()] += 1\n",
        "acc = 100*correct/total\n",
        "print(correct)\n",
        "print(total)\n",
        "print('Testing accuracy: ' + str(acc))\n",
        "# print(confusion_matrix)\n",
        "\n",
        "for i in range(2):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "acc0 = 100 * class_correct[0] / class_total[0]\n",
        "acc1 = 100 * class_correct[1] / class_total[1]\n",
        "print_row_end = []\n",
        "row_end = [acc,acc0,acc1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjDbMROBIPZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}